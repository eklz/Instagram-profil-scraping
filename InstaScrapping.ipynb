{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InstaScrapping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19qsi_JK0rijGmBxRRK5ufbBIMhTIVFTb",
      "authorship_tag": "ABX9TyNbb2E82eYQ6RIpVbPT64Sz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eklz/Prediction-de-likes-Instagram/blob/test/InstaScrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxz9FGY2CxY5",
        "colab_type": "text"
      },
      "source": [
        "###Instagram Scrapping\n",
        "\n",
        "The following code let us extract the data from Instagram to build our database. It is done with the help of Selenium and BeautifulSoup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7EiGOfbBo0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver #chromedriver gets install on the current hosted session in colab. needed to use selenium"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cvKw_uCBppw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from selenium import webdriver\n",
        "import datetime as dt\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import json\n",
        "import requests\n",
        "import json\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU15hD74XWSI",
        "colab_type": "code",
        "outputId": "93256aca-edad-401a-9f68-abe5cdac4a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = '/content/gdrive/My Drive/InstaScrap/'\n",
        "if not os.path.exists(root_path): #build the directory id it doesn't exist\n",
        "      os.makedirs(root_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6P0SdzuEC9s",
        "colab_type": "text"
      },
      "source": [
        "## The User class\n",
        "\n",
        "`loadData(self)` gets the Json file from the Java code of a given Instagram account. This Json file contains all the user informations and the informations of the last 12 posts.\n",
        "\n",
        "`loadPictures(self)` take a look at the last 12 posts and return only the pictures (no video or group of pictures) with the associated informations.\n",
        "\n",
        "`createUser(self)` returns a dictionnary with all of the informations needed : `{'name', 'nbPubli', \n",
        "        'nbFollowers', 'nbFollow', \n",
        "        'typeBusiness','latsPosts'}`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKxozxEZKR-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class User():\n",
        "\n",
        "  def __init__(self, username):\n",
        "\n",
        "    self.username=username\n",
        "    self.URL='http://www.instagram.com/'+username+'/?hl=fr'\n",
        "    self.data=self.loadData()\n",
        "    self.nbPubli= self.data['edge_owner_to_timeline_media']['count']\n",
        "    self.followed_by=self.data['edge_followed_by']['count']\n",
        "    self.follow=self.data['edge_follow']['count']\n",
        "    self.business=self.data['business_category_name']\n",
        "\n",
        "  def loadData(self):\n",
        "\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage') #in. order to run in colab\n",
        "\n",
        "    browser = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "    browser.get(self.URL)\n",
        "    source = browser.page_source\n",
        "    browser.quit()\n",
        "\n",
        "    data = bs(source, 'html.parser')\n",
        "    body = data.find('body')\n",
        "    script = body.find('script', text=lambda t: t.startswith('window._sharedData'))\n",
        "    page_json = script.text.split(' = ', 1)[1].rstrip(';')\n",
        "    data = json.loads(page_json)\n",
        "    data = data['entry_data']['ProfilePage'][0]['graphql']['user']\n",
        "    return data\n",
        "\n",
        "  def loadPictures(self):\n",
        "    \n",
        "    posts=[]\n",
        "    for publi in self.data['edge_owner_to_timeline_media']['edges']:\n",
        "      publi=publi['node']\n",
        "\n",
        "      if publi['__typename'] == 'GraphImage':\n",
        "\n",
        "        link=publi['shortcode']\n",
        "        analyse=publi['accessibility_caption']\n",
        "        comments=publi['edge_media_to_comment']['count']\n",
        "        likes=publi['edge_liked_by']['count']\n",
        "        try:\n",
        "          location_id=publi['location']['id']\n",
        "          location_name=publi['location']['name']\n",
        "        except:\n",
        "          location_id=None\n",
        "          location_name=None\n",
        "        timestamp=publi['taken_at_timestamp']\n",
        "        time=dt.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        display_url=publi['display_url']\n",
        "\n",
        "        carac={'link':link, 'likes':likes, 'comments':comments, \n",
        "               'location_id':location_id, 'location_name':location_name, \n",
        "               'time':time , 'analyse':analyse, 'display_url':display_url}\n",
        "        posts.append(carac)\n",
        "\n",
        "    return posts\n",
        "\n",
        "  def createUser(self):\n",
        "\n",
        "    dic={'name':self.username, 'nbPubli': self.nbPubli, \n",
        "        'nbFollowers':self.followed_by, 'nbFollow':self.follow, \n",
        "        'typeBusiness':self.business,'latsPosts':self.loadPictures()}\n",
        "\n",
        "    return dic\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r23Bpo3JFycU",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The file `users.txt` contains the list of the 50 biggest instagram accounts extracted from Wikipedia. The dataset is savec as a jsoon in `dataset.txt`.\n",
        "\n",
        "For my application I play with a much bigger Dataset of more than 1.500 Instagram accounts giving me more than 12.000 posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UXamMQZqPK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(root_path+'users.txt','r') as f:\n",
        "  text=f.readline()\n",
        "listUsers=text.split('\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elHG8Ej7rpQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset=[]\n",
        "for user in listUsers:\n",
        "  infl=User(user)\n",
        "  dataset.append(infl.createUser())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPsBnti4B7hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(root_path+'dataset.txt','w') as f:\n",
        "  json.dump(dataset,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkCYbOZvHCbU",
        "colab_type": "text"
      },
      "source": [
        "## Download the pictures associated with your dataset\n",
        "\n",
        "With the dictionnary given by `class User()` it's very. easy to download all of the associated pictures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4_3TyJrAcmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downloadPictures(dataset):\n",
        "  for user in dataset:\n",
        "    for post in user['lastPosts']:\n",
        "      r = requests.get(post['display_url'])\n",
        "        with open(root_path + user['nama'] + '_' + post['link'] + '.jpg', 'wb') as f:\n",
        "          f.write(r.content)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}